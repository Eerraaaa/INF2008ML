{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Loading IAM dataset\n",
    "\n",
    "# Path to the IAM dataset folder\n",
    "iam_dataset_path = r\"C:\\Users\\Carine\\Desktop\\data\\human\"\n",
    "\n",
    "# Get all subfolders (writers) and load images\n",
    "image_paths = []\n",
    "human_labels = []\n",
    "\n",
    "for subfolder in os.listdir(iam_dataset_path):\n",
    "    subfolder_path = os.path.join(iam_dataset_path, subfolder)\n",
    "    if os.path.isdir(subfolder_path):\n",
    "        for img_file in os.listdir(subfolder_path):\n",
    "            if img_file.endswith(\".png\"):\n",
    "                image_paths.append(os.path.join(subfolder_path, img_file))\n",
    "                human_labels.append(0)  # 0 = human\n",
    "\n",
    "# Read images in grayscale, resize, normalize\n",
    "human_images = []\n",
    "for img_path in image_paths:\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    img = cv2.resize(img, (128, 128))  # or any size you prefer\n",
    "    human_images.append(img)\n",
    "\n",
    "human_images = np.array(human_images, dtype=\"float32\") / 255.0\n",
    "human_labels = np.array(human_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 34577 images and saved to C:\\Users\\Carine\\Desktop\\data\\AI\\deepwriting_training_images\n",
      "Processed 705 images and saved to C:\\Users\\Carine\\Desktop\\data\\AI\\deepwriting_validation_images\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Processing DeepWriting Dataset\n",
    "\n",
    "# Paths to DeepWriting datasets\n",
    "deepwriting_train_path = r\"C:\\Users\\Carine\\Desktop\\data\\AI\\deepwriting_training.npz\"\n",
    "deepwriting_val_path = r\"C:\\Users\\Carine\\Desktop\\data\\AI\\deepwriting_validation.npz\"\n",
    "\n",
    "# Output folders for processed images\n",
    "train_output_path = r\"C:\\Users\\Carine\\Desktop\\data\\AI\\deepwriting_training_images\"\n",
    "val_output_path = r\"C:\\Users\\Carine\\Desktop\\data\\AI\\deepwriting_validation_images\"\n",
    "os.makedirs(train_output_path, exist_ok=True)\n",
    "os.makedirs(val_output_path, exist_ok=True)\n",
    "\n",
    "# Function to load, convert strokes to images, and save them\n",
    "def process_deepwriting(npz_path, output_folder):\n",
    "    data = np.load(npz_path, allow_pickle=True)\n",
    "    strokes = data[\"strokes\"]  # Extract stroke sequences\n",
    "\n",
    "    def strokes_to_image(stroke_data, img_size=128):\n",
    "        x = np.cumsum(stroke_data[:, 0])\n",
    "        y = np.cumsum(stroke_data[:, 1])\n",
    "\n",
    "        x = ((x - x.min()) / (x.max() - x.min()) * (img_size - 1)).astype(np.int32)\n",
    "        y = ((y - y.min()) / (y.max() - y.min()) * (img_size - 1)).astype(np.int32)\n",
    "\n",
    "        img = np.ones((img_size, img_size), dtype=np.uint8) * 255  # White background\n",
    "\n",
    "        for i in range(len(x) - 1):\n",
    "            cv2.line(img, (x[i], y[i]), (x[i+1], y[i+1]), 0, thickness=2)\n",
    "\n",
    "        return img\n",
    "\n",
    "    for idx, stroke_data in enumerate(strokes):\n",
    "        img = strokes_to_image(stroke_data)\n",
    "        cv2.imwrite(os.path.join(output_folder, f\"deepwriting_{idx}.png\"), img)\n",
    "\n",
    "    print(f\"Processed {len(strokes)} images and saved to {output_folder}\")\n",
    "\n",
    "# Process training and validation datasets\n",
    "process_deepwriting(deepwriting_train_path, train_output_path)\n",
    "process_deepwriting(deepwriting_val_path, val_output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Loading DeepWriting Dataset\n",
    "\n",
    "# Paths to converted DeepWriting images\n",
    "deepwriting_train_path = r\"C:\\Users\\Carine\\Desktop\\data\\AI\\deepwriting_training_images\"\n",
    "deepwriting_val_path = r\"C:\\Users\\Carine\\Desktop\\data\\AI\\deepwriting_validation_images\"\n",
    "\n",
    "deepwriting_image_paths = []\n",
    "ai_labels = []\n",
    "\n",
    "# Load training images\n",
    "for img_file in os.listdir(deepwriting_train_path):\n",
    "    if img_file.endswith(\".png\"):\n",
    "        deepwriting_image_paths.append(os.path.join(deepwriting_train_path, img_file))\n",
    "        ai_labels.append(1)  # Label 1 = AI-generated handwriting\n",
    "\n",
    "# Load validation images\n",
    "for img_file in os.listdir(deepwriting_val_path):\n",
    "    if img_file.endswith(\".png\"):\n",
    "        deepwriting_image_paths.append(os.path.join(deepwriting_val_path, img_file))\n",
    "        ai_labels.append(1)  # AI label\n",
    "\n",
    "# Read, resize, normalize AI images\n",
    "ai_images = []\n",
    "for img_path in deepwriting_image_paths:\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    img = cv2.resize(img, (128, 128))  # Standardizing size\n",
    "    ai_images.append(img)\n",
    "\n",
    "ai_images = np.array(ai_images, dtype=\"float32\") / 255.0\n",
    "ai_labels = np.array(ai_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Undersampling:\n",
      "Total samples: 36821\n",
      "Human samples: 1539\n",
      "AI samples: 35282\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Merge Before Undersampling\n",
    "\n",
    "# Merge human and AI images & labels\n",
    "all_images = np.concatenate((human_images, ai_images), axis=0)\n",
    "all_labels = np.concatenate((human_labels, ai_labels), axis=0)\n",
    "\n",
    "# Flatten image data for undersampling\n",
    "all_images_flat = all_images.reshape(all_images.shape[0], -1)\n",
    "\n",
    "# Print original dataset size\n",
    "print(\"Before Undersampling:\")\n",
    "print(\"Total samples:\", all_images_flat.shape[0])\n",
    "print(\"Human samples:\", sum(all_labels == 0))\n",
    "print(\"AI samples:\", sum(all_labels == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After Undersampling:\n",
      "Balanced Total samples: 3078\n",
      "Balanced Human samples: 1539\n",
      "Balanced AI samples: 1539\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Undersampling to Balance Data \n",
    "\n",
    "# Set undersampling strategy to match human sample count\n",
    "human_sample_count = sum(all_labels == 0)\n",
    "rus = RandomUnderSampler(sampling_strategy={0: human_sample_count, 1: human_sample_count}, random_state=42)\n",
    "\n",
    "X_balanced_flat, y_balanced = rus.fit_resample(all_images_flat, all_labels)\n",
    "\n",
    "# Reshape back to (128,128) images\n",
    "X_balanced = X_balanced_flat.reshape(-1, 128, 128)\n",
    "\n",
    "# Print new dataset size after undersampling\n",
    "print(\"\\nAfter Undersampling:\")\n",
    "print(\"Balanced Total samples:\", X_balanced.shape[0])\n",
    "print(\"Balanced Human samples:\", sum(y_balanced == 0))\n",
    "print(\"Balanced AI samples:\", sum(y_balanced == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Split:\n",
      "Training set: (2462, 128, 128)\n",
      "Validation set: (616, 128, 128)\n",
      "Human in train: 1231 AI in train: 1231\n",
      "Human in val: 308 AI in val: 308\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Split into 80% Training, 20% Validation\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_balanced, y_balanced,\n",
    "    test_size=0.2,       # 20% validation\n",
    "    random_state=42,\n",
    "    shuffle=True,\n",
    "    stratify=y_balanced  # Preserves class balance\n",
    ")\n",
    "\n",
    "print(\"\\nFinal Split:\")\n",
    "print(\"Training set:\", X_train.shape)\n",
    "print(\"Validation set:\", X_val.shape)\n",
    "print(\"Human in train:\", sum(y_train == 0), \"AI in train:\", sum(y_train == 1))\n",
    "print(\"Human in val:\", sum(y_val == 0), \"AI in val:\", sum(y_val == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Final balanced dataset saved at: C:\\Users\\Carine\\Desktop\\data\\human_vs_ai_dataset.npz\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Save Final Dataset\n",
    "# Set path for saving the dataset\n",
    "save_path = r\"C:\\Users\\Carine\\Desktop\\data\\human_vs_ai_dataset.npz\"\n",
    "\n",
    "# Save data as NPZ format\n",
    "np.savez_compressed(save_path, X_train=X_train, y_train=y_train, X_val=X_val, y_val=y_val)\n",
    "\n",
    "print(f\"\\n✅ Final balanced dataset saved at: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of overlapping samples: 0\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "\n",
    "# Compute hash of each image to check for duplicates\n",
    "train_hashes = {hashlib.md5(x.tobytes()).hexdigest() for x in X_train}\n",
    "val_hashes = {hashlib.md5(x.tobytes()).hexdigest() for x in X_val}\n",
    "\n",
    "# Intersection should be zero (i.e., no duplicates)\n",
    "data_leakage_check = train_hashes.intersection(val_hashes)\n",
    "print(f\"Number of overlapping samples: {len(data_leakage_check)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_rf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cross_val_score\n\u001b[1;32m----> 3\u001b[0m cv_scores \u001b[38;5;241m=\u001b[39m cross_val_score(\u001b[43mbest_rf\u001b[49m, X_train, y_train, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCross-validation Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean(cv_scores)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'best_rf' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv_scores = cross_val_score(best_rf, X_train, y_train, cv=5, scoring=\"accuracy\")\n",
    "print(f\"Cross-validation Accuracy: {np.mean(cv_scores) * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
